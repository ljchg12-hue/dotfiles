#!/usr/bin/env python3
"""
Auto Model Profiler
Ollama ì„¤ì¹˜ ëª¨ë¸ + CLI ëª¨ë¸ (Codex, Gemini) ìë™ íƒì§€ ë° í”„ë¡œíŒŒì¼ ìƒì„±
"""

import subprocess
import json
import os
import sys
import shutil

def get_priority(model_name):
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ ê²°ì •"""
    name_lower = model_name.lower()

    # ìš°ì„  ì‚¬ìš© ëª¨ë¸ë“¤ (ë†’ì€ ìš°ì„ ìˆœìœ„)
    priority_models = {
        'qwen3-vl:235b-cloud': 10,
        'deepseek-v3.1:671b-cloud': 10,
        'gpt-oss:120b-cloud': 9,
        'qwen3-coder:480b-cloud': 10,
        'kimi-k2:1t-cloud': 10,
        'exaone4.0:32b': 9,
        'llama3:70b': 8
    }

    # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
    if model_name in priority_models:
        return priority_models[model_name]

    # ë¶€ë¶„ ë§¤ì¹­
    if 'qwen3-vl' in name_lower and 'cloud' in name_lower:
        return 10
    elif 'deepseek-v3' in name_lower and 'cloud' in name_lower:
        return 10
    elif 'gpt-oss' in name_lower and 'cloud' in name_lower:
        return 9
    elif 'qwen3-coder' in name_lower and 'cloud' in name_lower:
        return 10
    elif 'kimi-k2' in name_lower and 'cloud' in name_lower:
        return 10
    elif 'exaone4' in name_lower or 'exaone:4' in name_lower:
        return 9
    elif 'llama3:70b' in name_lower or 'llama3' in name_lower and '70b' in name_lower:
        return 8
    elif 'gemini' in name_lower:
        return 7
    elif 'codex' in name_lower:
        return 7
    else:
        return 5  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„

def get_cli_models():
    """CLI ëª¨ë¸ ìë™ íƒì§€ (Codex, Gemini)"""
    cli_models = []

    # Codex CLI ê°ì§€
    codex_paths = [
        '/home/leejc5147/.npm-global/bin/codex',
        shutil.which('codex')
    ]

    for path in codex_paths:
        if path and os.path.exists(path):
            try:
                version_result = subprocess.run(
                    [path, '--version'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                version = version_result.stdout.strip().split('\n')[0]

                cli_models.append({
                    'name': 'codex-cli',
                    'optimal_for': ['ì½”ë”©', 'ë””ë²„ê¹…', 'ë¦¬íŒ©í† ë§', 'Python', 'JavaScript', 'ì•Œê³ ë¦¬ì¦˜', 'ë³µì¡í•œ ì‹œìŠ¤í…œ ì„¤ê³„'],
                    'benchmarks': {
                        'HumanEval': 96.8,
                        'MBPP': 94.5,
                        'ì½”ë”©ì†ë„': 98.0,
                        'GPT-5': 95.0,
                        'ì¶”ë¡ ': 96.0,
                        'MMLU': 92.0
                    },
                    'vram_required': 0,  # API ê¸°ë°˜
                    'priority': 10,  # ìµœê³  ìš°ì„ ìˆœìœ„ (GPT-5 + Premium)
                    'enabled': True,
                    'model_type': 'cli',
                    'cli_command': path,
                    'version': version,
                    'backend': 'OpenAI GPT-5 (Premium Tier)'
                })
                print(f"âœ… Codex CLI ê°ì§€: {path} ({version})")
                break
            except Exception as e:
                print(f"âš ï¸ Codex CLI í™•ì¸ ì‹¤íŒ¨ ({path}): {e}")

    # Gemini CLI ê°ì§€
    gemini_paths = [
        '/home/leejc5147/.nvm/versions/node/v20.19.5/bin/gemini',
        shutil.which('gemini')
    ]

    for path in gemini_paths:
        if path and os.path.exists(path):
            try:
                version_result = subprocess.run(
                    [path, '--version'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                version = version_result.stdout.strip().split('\n')[0]

                cli_models.append({
                    'name': 'gemini-cli',
                    'optimal_for': ['ë¹ ë¥¸ ì‘ë‹µ', 'ë‹¤êµ­ì–´', 'ì¼ë°˜ ì‘ì—…', 'ë²ˆì—­', 'ìš”ì•½', 'ë©€í‹°ëª¨ë‹¬', 'ì‹¤ì‹œê°„ ë¶„ì„'],
                    'benchmarks': {
                        'MMLU': 92.5,
                        'ë‹¤êµ­ì–´': 97.0,
                        'ì†ë„': 99.0,
                        'HumanEval': 90.5,
                        'ì¶”ë¡ ': 91.0,
                        'ë©€í‹°ëª¨ë‹¬': 94.0
                    },
                    'vram_required': 0,  # API ê¸°ë°˜
                    'priority': 10,  # ìµœê³  ìš°ì„ ìˆœìœ„ (Premium + Auto-Update)
                    'enabled': True,
                    'model_type': 'cli',
                    'cli_command': path,
                    'version': version,
                    'backend': 'Google Gemini (Premium Tier, Auto-Update)'
                })
                print(f"âœ… Gemini CLI ê°ì§€: {path} ({version})")
                break
            except Exception as e:
                print(f"âš ï¸ Gemini CLI í™•ì¸ ì‹¤íŒ¨ ({path}): {e}")

    return cli_models

def get_ollama_models():
    """Ollama ì„¤ì¹˜ëœ ëª¨ë¸ ìë™ íƒì§€"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, check=True)
    except FileNotFoundError:
        print("âš ï¸ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜: curl -fsSL https://ollama.com/install.sh | sh")
        return []  # ì˜¤ë¥˜ ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸ Ollama ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return []

    models = []

    for line in result.stdout.split('\n')[1:]:
        if line.strip():
            parts = line.split()
            if parts:
                model_name = parts[0]
                models.append({
                    'name': model_name,
                    'optimal_for': classify_model(model_name),
                    'benchmarks': get_benchmark_scores(model_name),
                    'vram_required': estimate_vram(model_name),
                    'priority': get_priority(model_name),
                    'enabled': True,
                    'model_type': 'ollama'
                })

    return models

def classify_model(name):
    """ëª¨ë¸ëª… ê¸°ë°˜ ëŠ¥ë ¥ ìë™ ë¶„ë¥˜"""
    name_lower = name.lower()

    if 'code' in name_lower or 'codex' in name_lower:
        return ['ì½”ë”©', 'ë””ë²„ê¹…', 'ë¦¬íŒ©í† ë§', 'Python', 'JavaScript']
    elif 'claude' in name_lower:
        return ['ë³µì¡í•œ ì¶”ë¡ ', 'ì¥ë¬¸ ë¶„ì„', 'ë©€í‹°ìŠ¤í… ì‘ì—…', 'ë¹„íŒì  ì‚¬ê³ ']
    elif 'gemini' in name_lower:
        return ['ë‹¤êµ­ì–´', 'ë¹ ë¥¸ ì‘ë‹µ', 'ë©€í‹°ëª¨ë‹¬', 'ì‹¤ì‹œê°„ ì •ë³´']
    elif 'qwen3-coder' in name_lower:
        return ['ì½”ë”©', 'ë””ë²„ê¹…', 'ì•Œê³ ë¦¬ì¦˜', 'ì‹œìŠ¤í…œ ì„¤ê³„', 'Python', 'C++']
    elif 'qwen3-vl' in name_lower or 'qwen-vl' in name_lower:
        return ['ë¹„ì „', 'ì´ë¯¸ì§€ ë¶„ì„', 'ë©€í‹°ëª¨ë‹¬', 'OCR', 'ì‹œê° ì¶”ë¡ ']
    elif 'qwen' in name_lower:
        return ['ë‹¤êµ­ì–´', 'ìˆ˜í•™', 'ì½”ë”©', 'ì¤‘êµ­ì–´']
    elif 'llama' in name_lower:
        return ['ì¼ë°˜ ì‘ì—…', 'ì°½ì˜ì  ê¸€ì“°ê¸°', 'ëŒ€í™”']
    elif 'deepseek' in name_lower:
        return ['ì½”ë”©', 'ìˆ˜í•™', 'ì¶”ë¡ ', 'ì•Œê³ ë¦¬ì¦˜', 'ì‹œìŠ¤í…œ ì„¤ê³„']
    elif 'gpt-oss' in name_lower or 'gpt' in name_lower:
        return ['ë²”ìš©', 'ì¶”ë¡ ', 'ì°½ì˜ì  ê¸€ì“°ê¸°', 'ëŒ€í™”', 'ë¶„ì„']
    elif 'kimi' in name_lower:
        return ['ì¥ë¬¸ ì²˜ë¦¬', 'ë³µì¡í•œ ì¶”ë¡ ', 'ë©€í‹°ìŠ¤í… ì‘ì—…', 'ëŒ€ê·œëª¨ ì»¨í…ìŠ¤íŠ¸']
    elif 'exaone' in name_lower:
        return ['í•œêµ­ì–´', 'ë‹¤êµ­ì–´', 'ë²”ìš©', 'ì¶”ë¡ ']
    elif 'mistral' in name_lower:
        return ['ë‹¤êµ­ì–´', 'ì¼ë°˜ ì‘ì—…', 'ë¹ ë¥¸ ì‘ë‹µ']
    else:
        return ['ì¼ë°˜ ì‘ì—…']

def get_benchmark_scores(model_name):
    """ê³µê°œ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜ ì¡°íšŒ"""
    # ì£¼ìš” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë² ì´ìŠ¤
    benchmark_db = {
        'claude': {
            'HumanEval': 92.0,
            'MMLU': 88.7,
            'ì¶”ë¡ ': 95.0,
            'GSM8K': 95.0
        },
        'codex': {
            'HumanEval': 72.0,
            'ì½”ë”©ì†ë„': 90.0,
            'MBPP': 75.0
        },
        'gemini': {
            'MMLU': 90.0,
            'ë‹¤êµ­ì–´': 95.0,
            'ì†ë„': 90.0,
            'HumanEval': 88.0
        },
        'qwen3-coder': {
            'HumanEval': 93.5,
            'MBPP': 91.0,
            'MMLU': 87.0,
            'GSM8K': 89.0,
            'ì½”ë”©ì†ë„': 95.0
        },
        'qwen3-vl': {
            'MMLU': 88.5,
            'ë¹„ì „': 96.0,
            'ë©€í‹°ëª¨ë‹¬': 94.0,
            'OCR': 92.0
        },
        'qwen': {
            'HumanEval': 85.0,
            'MMLU': 86.0,
            'ë‹¤êµ­ì–´': 92.0,
            'GSM8K': 88.0
        },
        'llama': {
            'MMLU': 82.0,
            'ì°½ì˜ì„±': 88.0,
            'HumanEval': 70.0
        },
        'deepseek-v3': {
            'HumanEval': 96.3,
            'MMLU': 91.5,
            'GSM8K': 94.8,
            'MBPP': 92.5,
            'ì¶”ë¡ ': 93.0
        },
        'deepseek': {
            'HumanEval': 89.0,
            'MMLU': 84.0,
            'GSM8K': 91.0
        },
        'gpt-oss': {
            'HumanEval': 89.5,
            'MMLU': 90.0,
            'GSM8K': 92.0,
            'ì°½ì˜ì„±': 91.0,
            'ì¶”ë¡ ': 89.0
        },
        'kimi-k2': {
            'MMLU': 92.0,
            'ì¶”ë¡ ': 94.0,
            'ì¥ë¬¸ì²˜ë¦¬': 98.0,
            'GSM8K': 93.5,
            'HumanEval': 88.0
        },
        'exaone': {
            'MMLU': 84.0,
            'í•œêµ­ì–´': 95.0,
            'HumanEval': 80.0,
            'GSM8K': 86.0
        },
        'mistral': {
            'MMLU': 81.0,
            'HumanEval': 65.0,
            'ì†ë„': 85.0
        }
    }
    
    name_lower = model_name.lower()
    
    for key, scores in benchmark_db.items():
        if key in name_lower:
            return scores
    
    return {}

def estimate_vram(model_name):
    """ëª¨ë¸ í¬ê¸° ê¸°ë°˜ í•„ìš” VRAM ì¶”ì • (GB)"""
    name_lower = model_name.lower()

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì¶œ (ëŒ€ìš©ëŸ‰ ëª¨ë¸ ìš°ì„ )
    if '1t' in name_lower or '1000b' in name_lower:
        return 512  # 1T íŒŒë¼ë¯¸í„°
    elif '671b' in name_lower:
        return 384  # 671B íŒŒë¼ë¯¸í„°
    elif '480b' in name_lower:
        return 288  # 480B íŒŒë¼ë¯¸í„°
    elif '235b' in name_lower:
        return 144  # 235B íŒŒë¼ë¯¸í„°
    elif '120b' in name_lower:
        return 80  # 120B íŒŒë¼ë¯¸í„°
    elif '70b' in name_lower or '72b' in name_lower:
        return 80
    elif '65b' in name_lower:
        return 72
    elif '32b' in name_lower or '34b' in name_lower:
        return 48
    elif '30b' in name_lower:
        return 48
    elif '13b' in name_lower or '14b' in name_lower:
        return 24
    elif '7b' in name_lower or '8b' in name_lower:
        return 16
    elif '3b' in name_lower:
        return 8
    else:
        return 16  # ê¸°ë³¸ê°’

def save_profile(models):
    """í”„ë¡œíŒŒì¼ JSON ì €ì¥"""
    output_file = 'models_profile.json'
    
    # ê¸°ì¡´ íŒŒì¼ ë°±ì—…
    if os.path.exists(output_file):
        backup_file = f"{output_file}.backup"
        os.rename(output_file, backup_file)
        print(f"ğŸ“¦ ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_file}")
    
    # ìƒˆ í”„ë¡œíŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(models, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… {len(models)}ê°œ ëª¨ë¸ í”„ë¡œíŒŒì¼ ì €ì¥: {output_file}")

def print_summary(models):
    """ëª¨ë¸ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š ëª¨ë¸ í”„ë¡œíŒŒì¼ ìš”ì•½")
    print("="*60)
    
    for model in models:
        print(f"\nğŸ¤– {model['name']}")
        print(f"   íŠ¹í™” ëŠ¥ë ¥: {', '.join(model['optimal_for'])}")
        
        if model['benchmarks']:
            print(f"   ë²¤ì¹˜ë§ˆí¬:")
            for bench, score in model['benchmarks'].items():
                print(f"     - {bench}: {score}")
        
        print(f"   í•„ìš” VRAM: {model['vram_required']}GB")
        print(f"   ìš°ì„ ìˆœìœ„: {model['priority']}/10")
        print(f"   í™œì„±í™”: {'âœ…' if model['enabled'] else 'âŒ'}")
    
    print("\n" + "="*60)

if __name__ == '__main__':
    print("ğŸ” ëª¨ë¸ í”„ë¡œíŒŒì¼ë§ ì‹œì‘...\n")

    # 1. CLI ëª¨ë¸ íƒì§€
    print("ğŸ“¡ CLI ëª¨ë¸ íƒì§€ ì¤‘...")
    cli_models = get_cli_models()

    if cli_models:
        print(f"âœ… {len(cli_models)}ê°œ CLI ëª¨ë¸ ë°œê²¬\n")
    else:
        print("âš ï¸ CLI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

    # 2. Ollama ëª¨ë¸ íƒì§€
    print("ğŸ“¡ Ollama ëª¨ë¸ íƒì§€ ì¤‘...")
    ollama_models = get_ollama_models()

    if ollama_models:
        print(f"âœ… {len(ollama_models)}ê°œ Ollama ëª¨ë¸ ë°œê²¬\n")
    else:
        print("âš ï¸ Ollama ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

    # 3. ëª¨ë¸ í•©ì¹˜ê¸°
    all_models = cli_models + ollama_models

    if not all_models:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("\në‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("\n[CLI ëª¨ë¸]")
        print("  npm install -g @anthropics/codex-cli")
        print("  npm install -g @google/gemini-cli")
        print("\n[Ollama ëª¨ë¸]")
        print("  curl -fsSL https://ollama.com/install.sh | sh")
        print("  ollama pull qwen3-coder:480b-cloud")
        print("  ollama pull deepseek-v3.1:671b-cloud")
        sys.exit(1)

    print(f"ğŸ“Š ì´ {len(all_models)}ê°œ ëª¨ë¸ ë°œê²¬ (CLI: {len(cli_models)}, Ollama: {len(ollama_models)})")

    # 4. í”„ë¡œíŒŒì¼ ì €ì¥
    save_profile(all_models)

    # 5. ìš”ì•½ ì¶œë ¥
    print_summary(all_models)

    print("\nğŸ‰ í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ!")
    print("\nâœ¨ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. models_profile.json í™•ì¸")
    print("  2. python3 smart_router.py 'ì§ˆë¬¸' ì‹¤í–‰")
    print("  3. python3 ensemble_executor.py 'ì§ˆë¬¸' ì‹¤í–‰")
    print("  4. Claude Code ìŠ¤í‚¬ì—ì„œ í™œìš©")
    print("\nğŸ’¡ íŒ:")
    print("  - CLI ëª¨ë¸ì€ API ë¹„ìš©ì´ ë°œìƒí•˜ì§€ë§Œ ë¹ ë¦…ë‹ˆë‹¤")
    print("  - Ollama ëª¨ë¸ì€ ë¬´ë£Œì§€ë§Œ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤")
    print("  - ë‘˜ì„ ì¡°í•©í•˜ë©´ ìµœê³ ì˜ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
