#!/usr/bin/env python3
"""
MCP Bridge
Claude Code MCP íˆ´ì„ Pythonì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë¸Œë¦¬ì§€
"""

import subprocess
import json
import sys

class MCPBridge:
    """
    MCP íˆ´ í˜¸ì¶œ ë˜í¼
    Claude Code í™˜ê²½ì—ì„œ ì§ì ‘ ì‚¬ìš©í•˜ê±°ë‚˜, ìŠ¤íƒ ë“œì–¼ë¡ ìœ¼ë¡œ CLI í˜¸ì¶œ
    """

    def __init__(self, use_direct_cli=True):
        """
        Args:
            use_direct_cli: Trueë©´ CLI ì§ì ‘ í˜¸ì¶œ, Falseë©´ í–¥í›„ MCP í”„ë¡œí† ì½œ ì‚¬ìš©
        """
        self.use_direct_cli = use_direct_cli
        self.codex_path = "/home/leejc5147/.npm-global/bin/codex"
        self.gemini_path = "/home/leejc5147/.nvm/versions/node/v20.19.5/bin/gemini"

    def ask_codex(self, prompt, timeout=60):
        """
        Codex CLI í˜¸ì¶œ (GPT ê¸°ë°˜ ì½”ë“œ íŠ¹í™”)

        Args:
            prompt: ì§ˆë¬¸/ëª…ë ¹
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)

        Returns:
            str: Codex ì‘ë‹µ
        """
        try:
            result = subprocess.run(
                [self.codex_path, prompt],
                capture_output=True,
                text=True,
                timeout=timeout
            )

            if result.returncode != 0:
                print(f"âš ï¸ Codex ê²½ê³ : {result.stderr[:200]}", file=sys.stderr)

            return result.stdout.strip()

        except subprocess.TimeoutExpired:
            print(f"âŒ Codex íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)", file=sys.stderr)
            return ""
        except FileNotFoundError:
            print(f"âŒ Codex CLIë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.codex_path}", file=sys.stderr)
            return ""
        except Exception as e:
            print(f"âŒ Codex ì˜¤ë¥˜: {e}", file=sys.stderr)
            return ""

    def ask_gemini(self, prompt, timeout=60):
        """
        Gemini CLI í˜¸ì¶œ (Gemini 2.5 Pro)

        Args:
            prompt: ì§ˆë¬¸/ëª…ë ¹
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)

        Returns:
            str: Gemini ì‘ë‹µ
        """
        try:
            result = subprocess.run(
                [self.gemini_path, prompt],
                capture_output=True,
                text=True,
                timeout=timeout
            )

            if result.returncode != 0:
                print(f"âš ï¸ Gemini ê²½ê³ : {result.stderr[:200]}", file=sys.stderr)

            return result.stdout.strip()

        except subprocess.TimeoutExpired:
            print(f"âŒ Gemini íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)", file=sys.stderr)
            return ""
        except FileNotFoundError:
            print(f"âŒ Gemini CLIë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.gemini_path}", file=sys.stderr)
            return ""
        except Exception as e:
            print(f"âŒ Gemini ì˜¤ë¥˜: {e}", file=sys.stderr)
            return ""

    def get_model_info(self):
        """
        CLI ëª¨ë¸ ì •ë³´ ì¡°íšŒ

        Returns:
            dict: ëª¨ë¸ ì •ë³´
        """
        info = {
            "codex": {
                "path": self.codex_path,
                "version": self._get_version(self.codex_path, "--version"),
                "type": "cli",
                "backend": "OpenAI GPT-5 (Premium Tier)",
                "optimal_for": ["ì½”ë”©", "ë””ë²„ê¹…", "ë¦¬íŒ©í† ë§", "ì•Œê³ ë¦¬ì¦˜", "ë³µì¡í•œ ì‹œìŠ¤í…œ ì„¤ê³„"]
            },
            "gemini": {
                "path": self.gemini_path,
                "version": self._get_version(self.gemini_path, "--version"),
                "type": "cli",
                "backend": "Google Gemini (Premium Tier, Auto-Update)",
                "optimal_for": ["ë¹ ë¥¸ ì‘ë‹µ", "ë‹¤êµ­ì–´", "ì¼ë°˜ ì‘ì—…", "ë²ˆì—­", "ë©€í‹°ëª¨ë‹¬", "ì‹¤ì‹œê°„ ë¶„ì„"]
            }
        }
        return info

    def _get_version(self, cli_path, version_flag):
        """CLI ë²„ì „ ì¡°íšŒ"""
        try:
            result = subprocess.run(
                [cli_path, version_flag],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.stdout.strip().split('\n')[0]
        except:
            return "unknown"

    def compare_models(self, prompt, models=['codex', 'gemini'], timeout=60):
        """
        ì—¬ëŸ¬ CLI ëª¨ë¸ì— ë™ì¼ ì§ˆë¬¸ ì „ì†¡ í›„ ë¹„êµ

        Args:
            prompt: ì§ˆë¬¸
            models: ë¹„êµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
            timeout: ê° ëª¨ë¸ íƒ€ì„ì•„ì›ƒ

        Returns:
            dict: {model_name: response}
        """
        results = {}

        for model in models:
            print(f"ğŸ”„ {model} ì‹¤í–‰ ì¤‘...", file=sys.stderr)

            if model == 'codex':
                results['codex'] = self.ask_codex(prompt, timeout)
            elif model == 'gemini':
                results['gemini'] = self.ask_gemini(prompt, timeout)
            else:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model}", file=sys.stderr)

        return results

    def smart_ask(self, prompt, task_type='auto', timeout=60):
        """
        ì‘ì—… ìœ í˜•ë³„ ìë™ ë¼ìš°íŒ…

        Args:
            prompt: ì§ˆë¬¸
            task_type: 'code', 'general', 'fast', 'auto'
            timeout: íƒ€ì„ì•„ì›ƒ

        Returns:
            tuple: (ì„ íƒëœ_ëª¨ë¸, ì‘ë‹µ)
        """
        # ìë™ ê°ì§€
        if task_type == 'auto':
            prompt_lower = prompt.lower()

            code_keywords = ['ì½”ë“œ', 'í•¨ìˆ˜', 'function', 'bug', 'debug', 'algorithm',
                           'python', 'javascript', 'java', 'c++', 'êµ¬í˜„', 'ë””ë²„ê¹…']

            if any(kw in prompt_lower for kw in code_keywords):
                task_type = 'code'
            else:
                task_type = 'general'

        # ë¼ìš°íŒ…
        if task_type == 'code':
            print("ğŸ¯ ì‘ì—… ìœ í˜•: ì½”ë”© â†’ Codex ì„ íƒ", file=sys.stderr)
            return ('codex', self.ask_codex(prompt, timeout))
        elif task_type in ['general', 'fast']:
            print("ğŸ¯ ì‘ì—… ìœ í˜•: ì¼ë°˜/ë¹ ë¥¸ ì‘ë‹µ â†’ Gemini ì„ íƒ", file=sys.stderr)
            return ('gemini', self.ask_gemini(prompt, timeout))
        else:
            print("âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” task_type, Gemini ì‚¬ìš©", file=sys.stderr)
            return ('gemini', self.ask_gemini(prompt, timeout))

    def chain_ask(self, prompts, models=None, timeout=60):
        """
        ìˆœì°¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì´ì „ ì¶œë ¥ â†’ ë‹¤ìŒ ì…ë ¥)

        Args:
            prompts: í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë‹¨ì¼ í”„ë¡¬í”„íŠ¸
            models: ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™)
            timeout: ê° ë‹¨ê³„ íƒ€ì„ì•„ì›ƒ

        Returns:
            list: ê° ë‹¨ê³„ë³„ (ëª¨ë¸, ì‘ë‹µ) íŠœí”Œ
        """
        if isinstance(prompts, str):
            prompts = [prompts]

        if models is None:
            models = ['gemini', 'codex']  # ê¸°ë³¸: Gemini ì´ˆì•ˆ â†’ Codex ìµœì í™”

        results = []
        previous_output = ""

        for i, prompt in enumerate(prompts):
            model = models[i % len(models)]

            # ì´ì „ ì¶œë ¥ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            if previous_output:
                full_prompt = f"{prompt}\n\n[ì´ì „ ë‹¨ê³„ ì¶œë ¥]\n{previous_output}"
            else:
                full_prompt = prompt

            print(f"\n{'='*60}", file=sys.stderr)
            print(f"ğŸ”— ì²´ì¸ ë‹¨ê³„ {i+1}/{len(prompts)}: {model}", file=sys.stderr)
            print(f"{'='*60}", file=sys.stderr)

            if model == 'codex':
                output = self.ask_codex(full_prompt, timeout)
            elif model == 'gemini':
                output = self.ask_gemini(full_prompt, timeout)
            else:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model}, Gemini ì‚¬ìš©", file=sys.stderr)
                output = self.ask_gemini(full_prompt, timeout)

            results.append((model, output))
            previous_output = output

        return results


def main():
    """CLI í…ŒìŠ¤íŠ¸"""
    import argparse

    parser = argparse.ArgumentParser(description='MCP Bridge CLI')
    parser.add_argument('command', choices=['codex', 'gemini', 'compare', 'smart', 'chain', 'info'])
    parser.add_argument('prompt', nargs='?', help='ì§ˆë¬¸/ëª…ë ¹')
    parser.add_argument('--task-type', default='auto', choices=['auto', 'code', 'general', 'fast'])
    parser.add_argument('--timeout', type=int, default=60, help='íƒ€ì„ì•„ì›ƒ (ì´ˆ)')

    args = parser.parse_args()

    bridge = MCPBridge()

    if args.command == 'info':
        info = bridge.get_model_info()
        print(json.dumps(info, indent=2, ensure_ascii=False))

    elif args.command == 'codex':
        if not args.prompt:
            print("âŒ prompt í•„ìš”", file=sys.stderr)
            sys.exit(1)
        print(bridge.ask_codex(args.prompt, args.timeout))

    elif args.command == 'gemini':
        if not args.prompt:
            print("âŒ prompt í•„ìš”", file=sys.stderr)
            sys.exit(1)
        print(bridge.ask_gemini(args.prompt, args.timeout))

    elif args.command == 'compare':
        if not args.prompt:
            print("âŒ prompt í•„ìš”", file=sys.stderr)
            sys.exit(1)
        results = bridge.compare_models(args.prompt, timeout=args.timeout)
        print(json.dumps(results, indent=2, ensure_ascii=False))

    elif args.command == 'smart':
        if not args.prompt:
            print("âŒ prompt í•„ìš”", file=sys.stderr)
            sys.exit(1)
        model, response = bridge.smart_ask(args.prompt, args.task_type, args.timeout)
        print(f"[ì„ íƒëœ ëª¨ë¸: {model}]")
        print(response)

    elif args.command == 'chain':
        if not args.prompt:
            print("âŒ prompt í•„ìš”", file=sys.stderr)
            sys.exit(1)
        results = bridge.chain_ask([args.prompt], timeout=args.timeout)
        for i, (model, output) in enumerate(results):
            print(f"\n{'='*60}")
            print(f"ë‹¨ê³„ {i+1}: {model}")
            print(f"{'='*60}")
            print(output)


if __name__ == '__main__':
    main()
